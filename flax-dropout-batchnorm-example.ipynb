{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c78fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad, jit, value_and_grad\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from typing import Sequence, Any\n",
    "\n",
    "# Create PRNGKey (PRNG State)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec0768",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a8cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def setup(self):\n",
    "        self.layer1 = nn.Dense(features=512)\n",
    "        self.dropout1 = nn.Dropout(rate=0.3)\n",
    "        self.norm1 = nn.BatchNorm()\n",
    "        \n",
    "        self.layer2 = nn.Dense(features=512)\n",
    "        self.dropout2 = nn.Dropout(rate=0.4)\n",
    "        self.norm2 = nn.BatchNorm()\n",
    "        \n",
    "        self.layer3 = nn.Dense(features=10)\n",
    "        \n",
    "    \n",
    "    def __call__(self, x, train:bool = True):\n",
    "        x = nn.relu(self.layer1(x))\n",
    "        x = self.dropout1(x, deterministic=not train)\n",
    "        x = self.norm1(x, use_running_average=not train)\n",
    "        x = nn.relu(self.layer2(x))\n",
    "        x = self.dropout2(x, deterministic=not train)\n",
    "        x = self.norm2(x, use_running_average=not train)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = MLP()\n",
    "\n",
    "# Using `init` and dummy_x to generate model parameters\n",
    "key, init_key = jax.random.split(key)\n",
    "dummy_x = jax.random.uniform(init_key, (784, ))\n",
    "\n",
    "key, init_key, drop_key = jax.random.split(key, 3)\n",
    "\n",
    "variables = model.init({\"params\": init_key, \"dropout\": drop_key}, dummy_x, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5dd71f4-74df-447e-9313-afe22d1c54ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['params', 'batch_stats'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef3def69-2701-408d-87bc-4e08e0c319f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['norm1', 'norm2'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables['batch_stats'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cac82690-1cd0-4f85-9984-84f89f6b730b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['mean', 'var'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables['batch_stats']['norm1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0264bdb-1aa2-48a7-9893-ace648c2c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, drop_key = jax.random.split(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2054946d-aa26-463d-a1a6-eab903de83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, non_trainable_params = model.apply(variables, dummy_x, train=True, rngs={\"dropout\": drop_key},\n",
    "                                      mutable=['batch_stats']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6630b06-2621-410b-a9b8-732922de2053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bd3cf5b-e5fa-437c-b839-684f42e8c1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['batch_stats'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_trainable_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549918f",
   "metadata": {},
   "source": [
    "# Optimizer and learning rate scheduler methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "995e0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "lr = 1e-3\n",
    "lr_decay_fn = optax.linear_schedule(\n",
    "        init_value=lr,\n",
    "        end_value=1e-5,\n",
    "        transition_steps=200,\n",
    ")\n",
    "\n",
    "optimizer = optax.adam(\n",
    "            learning_rate=lr_decay_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117dc38",
   "metadata": {},
   "source": [
    "# TrainState\n",
    "\n",
    "Encapsulate the state of the training process into one class and manage it uniformly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58319f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0da5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainState(train_state.TrainState):\n",
    "    batch_stats: flax.core.FrozenDict[str, Any]\n",
    "\n",
    "# state = train_state.TrainState.create(apply_fn=model.apply, params=variables[\"params\"], tx=optimizer,\n",
    "#                                      batch_stats=variables[\"batch_stats\"])\n",
    "\n",
    "state = CustomTrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=optimizer,\n",
    "    batch_stats=variables['batch_stats'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871483e",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7506c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler, SequentialSampler\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "class FlattenAndCast(object):  \n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "\n",
    "# DataLoader returns numpy array, not torch Tensor\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class JAXRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, rng_key):\n",
    "        self.data_source = data_source\n",
    "        self.rng_key = rng_key\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.rng_key, current_rng = jax.random.split(self.rng_key)\n",
    "        return iter(jax.random.permutation(current_rng, jnp.arange(len(self))).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc327e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(self, dataset, rng_key=None, batch_size=1,\n",
    "                 shuffle=False, **kwargs):\n",
    "        if shuffle:\n",
    "            sampler = JAXRandomSampler(dataset, rng_key)\n",
    "        else:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        super().__init__(dataset, batch_size, sampler=sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf51d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the help of torchvision and NumpyLoader\n",
    "mnist_dataset_train = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "key, loader_key = jax.random.split(key)\n",
    "train_loader = NumpyLoader(mnist_dataset_train, loader_key, batch_size=128, shuffle=True,\n",
    "                           num_workers=0, collate_fn=numpy_collate, drop_last=True)\n",
    "\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False, transform=FlattenAndCast())\n",
    "eval_loader = NumpyLoader(mnist_dataset_test, batch_size=128, shuffle=False, num_workers=0,\n",
    "                          collate_fn=numpy_collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e34244",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d4c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, x, y, dropout_key):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits, new_state = state.apply_fn({\"params\": params, \"batch_stats\": state.batch_stats},\n",
    "                                           x, train=True, rngs={\"dropout\": dropout_key}, mutable=[\"batch_stats\"])\n",
    "        \n",
    "        one_hot = jax.nn.one_hot(y, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, new_state\n",
    "\n",
    "    grad_fn = value_and_grad(loss_fn, has_aux=True)  # `value_and_grad` return the loss while performing a grad \n",
    "    (loss, new_state), grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads, batch_stats=new_state[\"batch_stats\"])\n",
    "    return new_state, loss\n",
    "\n",
    "jit_train_step = jit(train_step, donate_argnums=(0,))  # donate_argnums is used for buffer reuse, in which case the buffers for input and output states are reused\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, x):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "    \n",
    "    logits = state.apply_fn({\"params\":state.params, \"batch_stats\": state.batch_stats},\n",
    "                            x, train=False)\n",
    "    return jnp.argmax(logits, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2266b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, loader):\n",
    "    total_acc = 0.\n",
    "    total_num = 0.\n",
    "    for x, y in loader:\n",
    "        y_pred = apply_model(state, x)\n",
    "        total_num += len(x)\n",
    "        total_acc += jnp.sum(y_pred == y)\n",
    "    return total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffa18d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - batch_idx 0, loss 2.559518337249756, Training set acc 0.3179420530796051, eval set accuracy 0.31070002913475037\n",
      "Epoch 0 - batch_idx 100, loss 0.3981797695159912, Training set acc 0.9382011890411377, eval set accuracy 0.9367000460624695\n",
      "Epoch 0 - batch_idx 200, loss 0.29799991846084595, Training set acc 0.9520065784454346, eval set accuracy 0.9492000341415405\n",
      "Epoch 0 - batch_idx 300, loss 0.22030052542686462, Training set acc 0.9536759257316589, eval set accuracy 0.9513000249862671\n",
      "Epoch 0 - batch_idx 400, loss 0.22531506419181824, Training set acc 0.9540432095527649, eval set accuracy 0.950700044631958\n",
      "Epoch 1 - batch_idx 0, loss 0.2441655695438385, Training set acc 0.954594075679779, eval set accuracy 0.9508000612258911\n",
      "Epoch 1 - batch_idx 100, loss 0.14692620933055878, Training set acc 0.9552618265151978, eval set accuracy 0.9508000612258911\n",
      "Epoch 1 - batch_idx 200, loss 0.10268256068229675, Training set acc 0.9557124972343445, eval set accuracy 0.9513000249862671\n",
      "Epoch 1 - batch_idx 300, loss 0.2459377646446228, Training set acc 0.956013023853302, eval set accuracy 0.9520000219345093\n",
      "Epoch 1 - batch_idx 400, loss 0.15757295489311218, Training set acc 0.9565138220787048, eval set accuracy 0.9523000717163086\n",
      "Epoch 2 - batch_idx 0, loss 0.2567376494407654, Training set acc 0.9565972685813904, eval set accuracy 0.95250004529953\n",
      "Epoch 2 - batch_idx 100, loss 0.24193325638771057, Training set acc 0.9569478034973145, eval set accuracy 0.9530000686645508\n",
      "Epoch 2 - batch_idx 200, loss 0.2481696605682373, Training set acc 0.9573317766189575, eval set accuracy 0.9529000520706177\n",
      "Epoch 2 - batch_idx 300, loss 0.2532919645309448, Training set acc 0.9579160213470459, eval set accuracy 0.9532000422477722\n",
      "Epoch 2 - batch_idx 400, loss 0.17953674495220184, Training set acc 0.9583667516708374, eval set accuracy 0.953700065612793\n",
      "Epoch 3 - batch_idx 0, loss 0.26324784755706787, Training set acc 0.9586171507835388, eval set accuracy 0.9538000226020813\n",
      "Epoch 3 - batch_idx 100, loss 0.3597122132778168, Training set acc 0.9590678811073303, eval set accuracy 0.9544000625610352\n",
      "Epoch 3 - batch_idx 200, loss 0.13435618579387665, Training set acc 0.9593850374221802, eval set accuracy 0.954300045967102\n",
      "Epoch 3 - batch_idx 300, loss 0.18391358852386475, Training set acc 0.9598524570465088, eval set accuracy 0.9546000361442566\n",
      "Epoch 3 - batch_idx 400, loss 0.17724786698818207, Training set acc 0.9603699445724487, eval set accuracy 0.9552000164985657\n",
      "Epoch 4 - batch_idx 0, loss 0.23842677474021912, Training set acc 0.9603198766708374, eval set accuracy 0.955500066280365\n",
      "Epoch 4 - batch_idx 100, loss 0.13022586703300476, Training set acc 0.9607706069946289, eval set accuracy 0.9561000466346741\n",
      "Epoch 4 - batch_idx 200, loss 0.24015206098556519, Training set acc 0.9611212015151978, eval set accuracy 0.9568000435829163\n",
      "Epoch 4 - batch_idx 300, loss 0.20930230617523193, Training set acc 0.9615218043327332, eval set accuracy 0.9566000699996948\n",
      "Epoch 4 - batch_idx 400, loss 0.14273826777935028, Training set acc 0.9619224667549133, eval set accuracy 0.9566000699996948\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        key, dropout_key = jax.random.split(key)\n",
    "        state, loss = jit_train_step(state, x, y, dropout_key)\n",
    "        if idx % 100 == 0:  # evaluation\n",
    "            train_acc = eval_model(state, train_loader)\n",
    "            eval_acc = eval_model(state, eval_loader)\n",
    "            print(\"Epoch {} - batch_idx {}, loss {}, Training set acc {}, eval set accuracy {}\".format(\n",
    "              epoch, idx, loss, train_acc, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993476cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
