{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b408eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "# Create PRNGKey (PRNG State)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e24b6",
   "metadata": {},
   "source": [
    "# Model parameters and forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd324df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create model parameters, excluding input layer, actually there are three Linear layers, each layer contains a set of (w, b), for a total of three sets of parameters\n",
    "\n",
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "    \"\"\"\n",
    "    A helper function to randomly initialize weights and biases\n",
    "    for a dense neural network layer\n",
    "    \"\"\"\n",
    "    w_key, b_key = jax.random.split(key)  # Update explicitly PRNG state\n",
    "    return scale * jax.random.normal(w_key, (n, m)), scale * jax.random.normal(b_key, (n,))\n",
    "\n",
    "\n",
    "def init_network_params(sizes, key):\n",
    "    \"\"\"Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "    \"\"\"\n",
    "    keys = jax.random.split(key, len(sizes))  # split can create multiple keys at the same time\n",
    "    return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70e6b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 2 2 2\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [784, 512, 512, 10]\n",
    "\n",
    "key, init_key = jax.random.split(key)  # init_key used for initialization\n",
    "params = init_network_params(layer_sizes, init_key)\n",
    "\n",
    "print(len(params), len(params[0]), len(params[1]), len(params[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8faf8d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784) (512,)\n"
     ]
    }
   ],
   "source": [
    "print(params[0][0].shape, params[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e05880e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "# Creating a network is actually to write the forward pass\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "# Note that the x below is just an image，we don't need to implement batched_x\n",
    "def model_forward(params, x):\n",
    "    # per-example predictions\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.dot(w, x) + b\n",
    "        x = relu(x)\n",
    "    final_w, final_b = params[-1]\n",
    "    logits = jnp.dot(final_w, x) + final_b\n",
    "    return logits\n",
    "\n",
    "\n",
    "# forward has been completed, test it\n",
    "key, test_key = jax.random.split(key)\n",
    "random_flattened_image = jax.random.normal(test_key, (784, ))\n",
    "preds = model_forward(params, random_flattened_image)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033514fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random batch data, shape=(32, 784)\n",
    "random_batched_flattened_images = jax.random.normal(jax.random.PRNGKey(1), (32, 784))\n",
    "# model_forward(params, random_batched_flattened_images)  # error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f25144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10)\n"
     ]
    }
   ],
   "source": [
    "# create a model_forward which support batch data? just to use vmap, life is so easy\n",
    "batched_forward = vmap(model_forward, in_axes=(None, 0), out_axes=0)\n",
    "\n",
    "batched_preds = batched_forward(params, random_batched_flattened_images)\n",
    "print(batched_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7121d",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e619595d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler, SequentialSampler\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "# DataLoader returns numpy array，not torch Tensor\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class JAXRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, rng_key):\n",
    "        self.data_source = data_source\n",
    "        self.rng_key = rng_key\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.rng_key, current_rng = jax.random.split(self.rng_key)\n",
    "        return iter(jax.random.permutation(current_rng, jnp.arange(len(self))).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf3da509",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(self, dataset, rng_key=None, batch_size=1,\n",
    "                 shuffle=False, **kwargs):\n",
    "        if shuffle:\n",
    "            sampler = JAXRandomSampler(dataset, rng_key)\n",
    "        else:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        super().__init__(dataset, batch_size, sampler=sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db02505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the help of torchvision and NumpyLoader\n",
    "mnist_dataset_train = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "key, loader_key = jax.random.split(key)\n",
    "train_loader = NumpyLoader(mnist_dataset_train, loader_key, batch_size=32, shuffle=True,\n",
    "                           num_workers=0, collate_fn=numpy_collate, drop_last=True)\n",
    "\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False, transform=FlattenAndCast())\n",
    "eval_loader = NumpyLoader(mnist_dataset_test, batch_size=128, shuffle=False, num_workers=0,\n",
    "                          collate_fn=numpy_collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb550090",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba728b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logsumexp\n",
    "\n",
    "\n",
    "def loss(params, images, targets):\n",
    "    logits = batched_forward(params, images)\n",
    "    preds = logits - logsumexp(logits)\n",
    "    return -jnp.mean(preds * targets)\n",
    "\n",
    "\n",
    "@jit\n",
    "def sgd_update(params, x, y, lr):\n",
    "    grads = grad(loss)(params, x, y)\n",
    "    return jax.tree_util.tree_map(lambda p, g: p - lr * g, params, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c7fb13b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - batch_idx 0, Training set acc 0.09814999997615814, eval set accuracy 0.09950000047683716\n",
      "Epoch 0 - batch_idx 100, Training set acc 0.8302666544914246, eval set accuracy 0.8362999558448792\n",
      "Epoch 0 - batch_idx 200, Training set acc 0.8892666697502136, eval set accuracy 0.8940999507904053\n",
      "Epoch 0 - batch_idx 300, Training set acc 0.8997166752815247, eval set accuracy 0.9006999731063843\n",
      "Epoch 0 - batch_idx 400, Training set acc 0.9085167050361633, eval set accuracy 0.9128999710083008\n",
      "Epoch 0 - batch_idx 500, Training set acc 0.9076499938964844, eval set accuracy 0.911300003528595\n",
      "Epoch 0 - batch_idx 600, Training set acc 0.9230999946594238, eval set accuracy 0.9253000020980835\n",
      "Epoch 0 - batch_idx 700, Training set acc 0.9269000291824341, eval set accuracy 0.9298999905586243\n",
      "Epoch 0 - batch_idx 800, Training set acc 0.9295666813850403, eval set accuracy 0.9334999918937683\n",
      "Epoch 0 - batch_idx 900, Training set acc 0.9290666580200195, eval set accuracy 0.9296999573707581\n",
      "Epoch 0 - batch_idx 1000, Training set acc 0.9342833161354065, eval set accuracy 0.9357999563217163\n",
      "Epoch 0 - batch_idx 1100, Training set acc 0.9355999827384949, eval set accuracy 0.936199963092804\n",
      "Epoch 0 - batch_idx 1200, Training set acc 0.9371333718299866, eval set accuracy 0.9386000037193298\n",
      "Epoch 0 - batch_idx 1300, Training set acc 0.9381666779518127, eval set accuracy 0.9393999576568604\n",
      "Epoch 0 - batch_idx 1400, Training set acc 0.9390166997909546, eval set accuracy 0.9408999681472778\n",
      "Epoch 0 - batch_idx 1500, Training set acc 0.9398166537284851, eval set accuracy 0.9417999982833862\n",
      "Epoch 0 - batch_idx 1600, Training set acc 0.9398333430290222, eval set accuracy 0.9413999915122986\n",
      "Epoch 0 - batch_idx 1700, Training set acc 0.9414333701133728, eval set accuracy 0.9416999816894531\n",
      "Epoch 0 - batch_idx 1800, Training set acc 0.9421833157539368, eval set accuracy 0.9432999491691589\n",
      "Epoch 1 - batch_idx 0, Training set acc 0.9424499869346619, eval set accuracy 0.9432999491691589\n",
      "Epoch 1 - batch_idx 100, Training set acc 0.9426666498184204, eval set accuracy 0.9440000057220459\n",
      "Epoch 1 - batch_idx 200, Training set acc 0.9428333640098572, eval set accuracy 0.9438999891281128\n",
      "Epoch 1 - batch_idx 300, Training set acc 0.9431333541870117, eval set accuracy 0.943399965763092\n",
      "Epoch 1 - batch_idx 400, Training set acc 0.9421666860580444, eval set accuracy 0.9435999989509583\n",
      "Epoch 1 - batch_idx 500, Training set acc 0.9435333609580994, eval set accuracy 0.9447000026702881\n",
      "Epoch 1 - batch_idx 600, Training set acc 0.944350004196167, eval set accuracy 0.9447999596595764\n",
      "Epoch 1 - batch_idx 700, Training set acc 0.944599986076355, eval set accuracy 0.9444999694824219\n",
      "Epoch 1 - batch_idx 800, Training set acc 0.9443833231925964, eval set accuracy 0.9451999664306641\n",
      "Epoch 1 - batch_idx 900, Training set acc 0.9438666701316833, eval set accuracy 0.9442999958992004\n",
      "Epoch 1 - batch_idx 1000, Training set acc 0.944433331489563, eval set accuracy 0.9460999965667725\n",
      "Epoch 1 - batch_idx 1100, Training set acc 0.9442333579063416, eval set accuracy 0.9458999633789062\n",
      "Epoch 1 - batch_idx 1200, Training set acc 0.9454500079154968, eval set accuracy 0.9459999799728394\n",
      "Epoch 1 - batch_idx 1300, Training set acc 0.9456000328063965, eval set accuracy 0.946899950504303\n",
      "Epoch 1 - batch_idx 1400, Training set acc 0.9452000260353088, eval set accuracy 0.9457999467849731\n",
      "Epoch 1 - batch_idx 1500, Training set acc 0.9455666542053223, eval set accuracy 0.9472999572753906\n",
      "Epoch 1 - batch_idx 1600, Training set acc 0.9462500214576721, eval set accuracy 0.9473999738693237\n",
      "Epoch 1 - batch_idx 1700, Training set acc 0.9463333487510681, eval set accuracy 0.9470999836921692\n",
      "Epoch 1 - batch_idx 1800, Training set acc 0.9464666843414307, eval set accuracy 0.9473999738693237\n",
      "Epoch 2 - batch_idx 0, Training set acc 0.9475666880607605, eval set accuracy 0.946899950504303\n",
      "Epoch 2 - batch_idx 100, Training set acc 0.9472500085830688, eval set accuracy 0.9470999836921692\n",
      "Epoch 2 - batch_idx 200, Training set acc 0.9470500349998474, eval set accuracy 0.9469999670982361\n",
      "Epoch 2 - batch_idx 300, Training set acc 0.9470500349998474, eval set accuracy 0.948199987411499\n",
      "Epoch 2 - batch_idx 400, Training set acc 0.9472500085830688, eval set accuracy 0.9477999806404114\n",
      "Epoch 2 - batch_idx 500, Training set acc 0.9465166926383972, eval set accuracy 0.9478999972343445\n",
      "Epoch 2 - batch_idx 600, Training set acc 0.9469666481018066, eval set accuracy 0.9478999972343445\n",
      "Epoch 2 - batch_idx 700, Training set acc 0.948116660118103, eval set accuracy 0.9478999972343445\n",
      "Epoch 2 - batch_idx 800, Training set acc 0.9487500190734863, eval set accuracy 0.9479999542236328\n",
      "Epoch 2 - batch_idx 900, Training set acc 0.9488500356674194, eval set accuracy 0.9477999806404114\n",
      "Epoch 2 - batch_idx 1000, Training set acc 0.9477333426475525, eval set accuracy 0.9480999708175659\n",
      "Epoch 2 - batch_idx 1100, Training set acc 0.948033332824707, eval set accuracy 0.9473999738693237\n",
      "Epoch 2 - batch_idx 1200, Training set acc 0.9485999941825867, eval set accuracy 0.9498999714851379\n",
      "Epoch 2 - batch_idx 1300, Training set acc 0.9485999941825867, eval set accuracy 0.9498999714851379\n",
      "Epoch 2 - batch_idx 1400, Training set acc 0.949316680431366, eval set accuracy 0.9490000009536743\n",
      "Epoch 2 - batch_idx 1500, Training set acc 0.9492999911308289, eval set accuracy 0.9492999911308289\n",
      "Epoch 2 - batch_idx 1600, Training set acc 0.9499500393867493, eval set accuracy 0.9493999481201172\n",
      "Epoch 2 - batch_idx 1700, Training set acc 0.9496666789054871, eval set accuracy 0.9501000046730042\n",
      "Epoch 2 - batch_idx 1800, Training set acc 0.9499833583831787, eval set accuracy 0.9503999948501587\n",
      "Epoch 3 - batch_idx 0, Training set acc 0.9501000046730042, eval set accuracy 0.9492999911308289\n",
      "Epoch 3 - batch_idx 100, Training set acc 0.9503166675567627, eval set accuracy 0.9497999548912048\n",
      "Epoch 3 - batch_idx 200, Training set acc 0.9498666524887085, eval set accuracy 0.9496999979019165\n",
      "Epoch 3 - batch_idx 300, Training set acc 0.9505333304405212, eval set accuracy 0.9490999579429626\n",
      "Epoch 3 - batch_idx 400, Training set acc 0.9503333568572998, eval set accuracy 0.9506999850273132\n",
      "Epoch 3 - batch_idx 500, Training set acc 0.951116681098938, eval set accuracy 0.950499951839447\n",
      "Epoch 3 - batch_idx 600, Training set acc 0.9506500363349915, eval set accuracy 0.950499951839447\n",
      "Epoch 3 - batch_idx 700, Training set acc 0.9513999819755554, eval set accuracy 0.949999988079071\n",
      "Epoch 3 - batch_idx 800, Training set acc 0.9507666826248169, eval set accuracy 0.9505999684333801\n",
      "Epoch 3 - batch_idx 900, Training set acc 0.9516333341598511, eval set accuracy 0.9501000046730042\n",
      "Epoch 3 - batch_idx 1000, Training set acc 0.9512333273887634, eval set accuracy 0.950499951839447\n",
      "Epoch 3 - batch_idx 1100, Training set acc 0.951366662979126, eval set accuracy 0.9514999985694885\n",
      "Epoch 3 - batch_idx 1200, Training set acc 0.9510833621025085, eval set accuracy 0.9506999850273132\n",
      "Epoch 3 - batch_idx 1300, Training set acc 0.9514999985694885, eval set accuracy 0.9515999555587769\n",
      "Epoch 3 - batch_idx 1400, Training set acc 0.9515500068664551, eval set accuracy 0.9503999948501587\n",
      "Epoch 3 - batch_idx 1500, Training set acc 0.9522833228111267, eval set accuracy 0.9512999653816223\n",
      "Epoch 3 - batch_idx 1600, Training set acc 0.9523333311080933, eval set accuracy 0.95169997215271\n",
      "Epoch 3 - batch_idx 1700, Training set acc 0.9521000385284424, eval set accuracy 0.9513999819755554\n",
      "Epoch 3 - batch_idx 1800, Training set acc 0.9531000256538391, eval set accuracy 0.9515999555587769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def one_hot(x, k=10, dtype=jnp.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "def accuracy(params, loader):\n",
    "    total_acc = 0\n",
    "    total_num = 0\n",
    "    for x, y in loader:\n",
    "        predicted_class = jnp.argmax(batched_forward(params, x), axis=1)\n",
    "        total_num += len(x)\n",
    "        total_acc += jnp.sum(predicted_class == y)\n",
    "    return total_acc / total_num\n",
    "\n",
    "\n",
    "lr = 0.01\n",
    "n_classes = 10\n",
    "for epoch in range(5):\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        y = one_hot(y, n_classes)\n",
    "        params = sgd_update(params, x, y, lr)\n",
    "        lr = lr * 0.999 if lr > 1e-3 else 1e-3  # very simple lr scheduler\n",
    "        if idx % 100 == 0:  # evaluation\n",
    "            train_acc = accuracy(params, train_loader)\n",
    "            eval_acc = accuracy(params, eval_loader)\n",
    "            print(\"Epoch {} - batch_idx {}, Training set acc {}, eval set accuracy {}\".format(\n",
    "                  epoch, idx, train_acc, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a0fd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
