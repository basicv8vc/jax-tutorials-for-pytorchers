{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c78fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad, pmap, value_and_grad\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from typing import Sequence, Any\n",
    "\n",
    "# 创建 PRNGKey (PRNG State)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec0768",
   "metadata": {},
   "source": [
    "# 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a8cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \n",
    "    def setup(self):\n",
    "        self.layer1 = nn.Dense(features=512)\n",
    "        self.dropout1 = nn.Dropout(rate=0.3)\n",
    "        self.norm1 = nn.BatchNorm()\n",
    "        \n",
    "        self.layer2 = nn.Dense(features=512)\n",
    "        self.dropout2 = nn.Dropout(rate=0.4)\n",
    "        self.norm2 = nn.BatchNorm()\n",
    "        \n",
    "        self.layer3 = nn.Dense(features=10)\n",
    "        \n",
    "    \n",
    "    def __call__(self, x, train:bool = True):\n",
    "        x = nn.relu(self.layer1(x))\n",
    "        x = self.dropout1(x, deterministic=not train)\n",
    "        x = self.norm1(x, use_running_average=not train)\n",
    "        x = nn.relu(self.layer2(x))\n",
    "        x = self.dropout2(x, deterministic=not train)\n",
    "        x = self.norm2(x, use_running_average=not train)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "model = MLP()\n",
    "\n",
    "# 使用`init`和dummy_x来创建模型参数\n",
    "key, init_key = jax.random.split(key)\n",
    "dummy_x = jax.random.uniform(init_key, (784, ))\n",
    "\n",
    "key, init_key, drop_key = jax.random.split(key, 3)\n",
    "\n",
    "variables = model.init({\"params\": init_key, \"dropout\": drop_key}, dummy_x, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2054946d-aa26-463d-a1a6-eab903de83c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, non_trainable_params = model.apply(variables, dummy_x, train=True, rngs={\"dropout\": drop_key},\n",
    "                                      mutable=['batch_stats']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6630b06-2621-410b-a9b8-732922de2053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bd3cf5b-e5fa-437c-b839-684f42e8c1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozen_dict_keys(['batch_stats'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_trainable_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871483e",
   "metadata": {},
   "source": [
    "# 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7506c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler, SequentialSampler\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "class FlattenAndCast(object):  \n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "\n",
    "# DataLoader返回numpy array，而不是torch Tensor\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class JAXRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, rng_key):\n",
    "        self.data_source = data_source\n",
    "        self.rng_key = rng_key\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.rng_key, current_rng = jax.random.split(self.rng_key)\n",
    "        return iter(jax.random.permutation(current_rng, jnp.arange(len(self))).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc327e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(self, dataset, rng_key=None, batch_size=1,\n",
    "                 shuffle=False, **kwargs):\n",
    "        if shuffle:\n",
    "            sampler = JAXRandomSampler(dataset, rng_key)\n",
    "        else:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        super().__init__(dataset, batch_size, sampler=sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf51d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 借助于torchvision和NumpyLoader\n",
    "mnist_dataset_train = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "key, loader_key = jax.random.split(key)\n",
    "train_loader = NumpyLoader(mnist_dataset_train, loader_key, batch_size=128 * jax.local_device_count(), shuffle=True,\n",
    "                           num_workers=0, collate_fn=numpy_collate, drop_last=True)\n",
    "\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False, transform=FlattenAndCast())\n",
    "eval_loader = NumpyLoader(mnist_dataset_test, batch_size=128 * jax.local_device_count(), shuffle=False, num_workers=0,\n",
    "                          collate_fn=numpy_collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c007046f",
   "metadata": {},
   "source": [
    "# 优化器和学习率调度算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e6e4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "lr = 1e-3\n",
    "lr_decay_fn = optax.linear_schedule(\n",
    "        init_value=lr,\n",
    "        end_value=1e-5,\n",
    "        transition_steps=200,\n",
    ")\n",
    "\n",
    "optimizer = optax.adam(\n",
    "            learning_rate=lr_decay_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b0d62",
   "metadata": {},
   "source": [
    "# TrainState\n",
    "\n",
    "将训练过程中的状态封装为一个类，统一管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03e65176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "class CustomTrainState(train_state.TrainState):\n",
    "    batch_stats: flax.core.FrozenDict[str, Any]\n",
    "\n",
    "state = CustomTrainState.create(\n",
    "    apply_fn=model.apply,\n",
    "    params=variables['params'],\n",
    "    tx=optimizer,\n",
    "    batch_stats=variables['batch_stats'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e34244",
   "metadata": {},
   "source": [
    "# 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, x, y, dropout_key):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits, new_state = state.apply_fn({\"params\": params, \"batch_stats\": state.batch_stats},\n",
    "                                           x, train=True, rngs={\"dropout\": dropout_key}, mutable=[\"batch_stats\"])\n",
    "        \n",
    "        one_hot = jax.nn.one_hot(y, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss, new_state\n",
    "\n",
    "    grad_fn = value_and_grad(loss_fn, has_aux=True)  # `value_and_grad`在进行grad同时返回loss\n",
    "    (loss, new_state), grads = grad_fn(state.params)\n",
    "    grads = jax.lax.pmean(grads, \"batch\")  # pmean计算所有device上的梯度均值\n",
    "    loss = jax.lax.pmean(loss, \"batch\")\n",
    "    batch_stats = jax.lax.pmean(new_state[\"batch_stats\"], \"batch\")\n",
    "    new_state = state.apply_gradients(grads=grads, batch_stats=batch_stats)\n",
    "    \n",
    "    return new_state, loss\n",
    "\n",
    "p_train_step = pmap(train_step, \"batch\", donate_argnums=(0,))  # donate_argnums用于buffer复用，这里指的是输入和输出的state buffer复用\n",
    "\n",
    "\n",
    "def apply_model(state, x):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "    \n",
    "    logits = state.apply_fn({\"params\":state.params, \"batch_stats\": state.batch_stats},\n",
    "                            x, train=False)\n",
    "    return jnp.argmax(logits, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2266b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, loader):\n",
    "    total_acc = 0.\n",
    "    total_num = 0.\n",
    "    for xs, ys in loader:\n",
    "        xs = jax.tree_map(\n",
    "            lambda x: x.reshape((jax.local_device_count(), -1) + x.shape[1:]), xs)\n",
    "        ys = jax.tree_map(\n",
    "            lambda x: x.reshape((jax.local_device_count(), -1) + x.shape[1:]), ys)\n",
    "        y_pred = pmap(apply_model)(state, xs)  # 验证时没有跨设备通信操作，不需要设置axis_name\n",
    "        total_num += ys.size\n",
    "        total_acc += jnp.sum(y_pred == ys)\n",
    "    return total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2976a36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = jax.local_devices()\n",
    "state = jax.device_put_replicated(state, devices)  # 或者 state = flax.jax_utils.replicate(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffa18d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - batch_idx 0, loss 2.666304588317871, Training set acc 0.4732118844985962, eval set accuracy 0.4770999848842621\n",
      "Epoch 1 - batch_idx 0, loss 0.20269665122032166, Training set acc 0.9528387784957886, eval set accuracy 0.9491999745368958\n",
      "Epoch 2 - batch_idx 0, loss 0.1484561264514923, Training set acc 0.9685648083686829, eval set accuracy 0.964199960231781\n",
      "Epoch 3 - batch_idx 0, loss 0.12777751684188843, Training set acc 0.9755185842514038, eval set accuracy 0.9678999781608582\n",
      "Epoch 4 - batch_idx 0, loss 0.10267762094736099, Training set acc 0.9769666194915771, eval set accuracy 0.9679999947547913\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for idx, (xs, ys) in enumerate(train_loader):\n",
    "        xs = jax.tree_map(\n",
    "            lambda x: x.reshape((jax.local_device_count(), -1) + x.shape[1:]), xs)\n",
    "        ys = jax.tree_map(\n",
    "            lambda x: x.reshape((jax.local_device_count(), -1) + x.shape[1:]), ys)\n",
    "        \n",
    "        key, dropout_key = jax.random.split(key)\n",
    "        dropout_key = jax.random.split(drop_key, jax.local_device_count())\n",
    "        state, loss = p_train_step(state, xs, ys, dropout_key)\n",
    "        \n",
    "        if idx % 100 == 0:  # evaluation\n",
    "            train_acc = eval_model(state, train_loader)\n",
    "            eval_acc = eval_model(state, eval_loader)\n",
    "            print(\"Epoch {} - batch_idx {}, loss {}, Training set acc {}, eval set accuracy {}\".format(\n",
    "              epoch, idx, jax.tree_map(lambda x: x[0], loss), train_acc, eval_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00080986",
   "metadata": {},
   "source": [
    "## 使用单设备验证"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073cca75",
   "metadata": {},
   "source": [
    "如果在模型验证时不想使用多设备，取其中一份state，jax.tree_util.tree_map(lambda x: x[0], state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f20c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_single(state, loader):\n",
    "    total_acc = 0.\n",
    "    total_num = 0.\n",
    "    for xs, ys in loader:\n",
    "        y_pred = jax.jit(apply_model)(state, xs)\n",
    "        total_num += ys.size\n",
    "        total_acc += jnp.sum(y_pred == ys)\n",
    "    return total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47bf85a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(0.96809995, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model_single(jax.tree_util.tree_map(lambda x: x[0], state), eval_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd258305",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
