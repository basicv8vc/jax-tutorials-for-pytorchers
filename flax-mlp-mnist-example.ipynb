{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1c78fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad, jit, value_and_grad\n",
    "from flax import linen as nn\n",
    "from typing import Sequence\n",
    "\n",
    "# Create PRNGKey (PRNG State)\n",
    "key = jax.random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec0768",
   "metadata": {},
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a8cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    layer_sizes: Sequence[int] = None\n",
    "    \n",
    "    def setup(self):\n",
    "        self.layers = [nn.Dense(features=size) for size in self.layer_sizes[1:]]\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            x = nn.relu(x)\n",
    "        return self.layers[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [784, 512, 512, 10]\n",
    "\n",
    "# Create model\n",
    "model = MLP(layer_sizes)\n",
    "\n",
    "# Using `init` and dummy_x to create model parameters\n",
    "key, init_key = jax.random.split(key)  # init_key used for initialization\n",
    "dummy_x = jax.random.uniform(init_key, (784, ))\n",
    "key, init_key = jax.random.split(key)\n",
    "\n",
    "params = model.init(init_key, dummy_x)\n",
    "\n",
    "# params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c549918f",
   "metadata": {},
   "source": [
    "# Optimizer and learning rate scheduler methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "995e0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "\n",
    "lr = 1e-3\n",
    "lr_decay_fn = optax.linear_schedule(\n",
    "        init_value=lr,\n",
    "        end_value=1e-5,\n",
    "        transition_steps=200,\n",
    ")\n",
    "\n",
    "optimizer = optax.adam(\n",
    "            learning_rate=lr_decay_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16321610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creat a random batch data, shape=(32, 784)\n",
    "random_batched_flattened_images = jax.random.normal(jax.random.PRNGKey(1), (32, 784))\n",
    "\n",
    "model.apply(params, random_batched_flattened_images).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117dc38",
   "metadata": {},
   "source": [
    "# TrainState\n",
    "\n",
    "Encapsulate the state of the training process into one class and manage it uniformly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58319f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax.training import train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0da5f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871483e",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7506c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Sampler, SequentialSampler\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "class FlattenAndCast(object):  \n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))\n",
    "\n",
    "\n",
    "# DataLoader returns numpy arrayï¼Œnot torch Tensor\n",
    "def numpy_collate(batch):\n",
    "    if isinstance(batch[0], np.ndarray):\n",
    "        return np.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [numpy_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return np.array(batch)\n",
    "\n",
    "class JAXRandomSampler(Sampler):\n",
    "    def __init__(self, data_source, rng_key):\n",
    "        self.data_source = data_source\n",
    "        self.rng_key = rng_key\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.rng_key, current_rng = jax.random.split(self.rng_key)\n",
    "        return iter(jax.random.permutation(current_rng, jnp.arange(len(self))).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc327e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLoader(DataLoader):\n",
    "    def __init__(self, dataset, rng_key=None, batch_size=1,\n",
    "                 shuffle=False, **kwargs):\n",
    "        if shuffle:\n",
    "            sampler = JAXRandomSampler(dataset, rng_key)\n",
    "        else:\n",
    "            sampler = SequentialSampler(dataset)\n",
    "        \n",
    "        super().__init__(dataset, batch_size, sampler=sampler, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf51d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the help of torchvision and NumpyLoader\n",
    "mnist_dataset_train = MNIST('/tmp/mnist/', download=True, transform=FlattenAndCast())\n",
    "key, loader_key = jax.random.split(key)\n",
    "train_loader = NumpyLoader(mnist_dataset_train, loader_key, batch_size=128, shuffle=True,\n",
    "                           num_workers=0, collate_fn=numpy_collate, drop_last=True)\n",
    "\n",
    "mnist_dataset_test = MNIST('/tmp/mnist/', download=True, train=False, transform=FlattenAndCast())\n",
    "eval_loader = NumpyLoader(mnist_dataset_test, batch_size=128, shuffle=False, num_workers=0,\n",
    "                          collate_fn=numpy_collate, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e34244",
   "metadata": {},
   "source": [
    "# Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d4c5586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, x, y):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn(params, x)\n",
    "        one_hot = jax.nn.one_hot(y, 10)\n",
    "        loss = jnp.mean(optax.softmax_cross_entropy(logits=logits, labels=one_hot))\n",
    "        return loss\n",
    "\n",
    "    grad_fn = value_and_grad(loss_fn)  # `value_and_grad` return the loss while performing a grad \n",
    "    loss, grads = grad_fn(state.params)\n",
    "    new_state = state.apply_gradients(grads=grads)\n",
    "    return new_state, loss\n",
    "\n",
    "jit_train_step = jit(train_step, donate_argnums=(0,))  # donate_argnums is used for buffer reuse, in which case the buffers for input and output states are reused\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def apply_model(state, x):\n",
    "    \"\"\"Computes gradients and loss for a single batch.\"\"\"\n",
    "    \n",
    "    logits = state.apply_fn(state.params, x)\n",
    "    return jnp.argmax(logits, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2266b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(state, loader):\n",
    "    total_acc = 0.\n",
    "    total_num = 0.\n",
    "    for x, y in loader:\n",
    "        y_pred = apply_model(state, x)\n",
    "        total_num += len(x)\n",
    "        total_acc += jnp.sum(y_pred == y)\n",
    "    return total_acc / total_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa18d03",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - batch_idx 0, loss 59.9576530456543, Training set acc 0.28944647312164307, eval set accuracy 0.2947999835014343\n",
      "Epoch 0 - batch_idx 100, loss 0.8403108716011047, Training set acc 0.9234442114830017, eval set accuracy 0.9197999835014343\n",
      "Epoch 0 - batch_idx 200, loss 0.11547716706991196, Training set acc 0.938969075679779, eval set accuracy 0.9287999868392944\n",
      "Epoch 0 - batch_idx 300, loss 1.2561315298080444, Training set acc 0.9425748586654663, eval set accuracy 0.9311999678611755\n",
      "Epoch 0 - batch_idx 400, loss 0.6336281895637512, Training set acc 0.9441940784454346, eval set accuracy 0.9339999556541443\n",
      "Epoch 1 - batch_idx 0, loss 0.5855568647384644, Training set acc 0.9455128908157349, eval set accuracy 0.9337999820709229\n",
      "Epoch 1 - batch_idx 100, loss 0.5183431506156921, Training set acc 0.9467315077781677, eval set accuracy 0.9347999691963196\n",
      "Epoch 1 - batch_idx 200, loss 0.22533944249153137, Training set acc 0.9475494623184204, eval set accuracy 0.9355999827384949\n",
      "Epoch 1 - batch_idx 300, loss 0.9593722224235535, Training set acc 0.9479501247406006, eval set accuracy 0.9345999956130981\n",
      "Epoch 1 - batch_idx 400, loss 0.3599521517753601, Training set acc 0.9491186141967773, eval set accuracy 0.9353999495506287\n",
      "Epoch 2 - batch_idx 0, loss 0.6886149048805237, Training set acc 0.9501703381538391, eval set accuracy 0.9357999563217163\n",
      "Epoch 2 - batch_idx 100, loss 0.9413076639175415, Training set acc 0.9508046507835388, eval set accuracy 0.9363999962806702\n",
      "Epoch 2 - batch_idx 200, loss 0.27974066138267517, Training set acc 0.9517895579338074, eval set accuracy 0.9364999532699585\n",
      "Epoch 2 - batch_idx 300, loss 0.27733081579208374, Training set acc 0.9522235989570618, eval set accuracy 0.9362999796867371\n",
      "Epoch 2 - batch_idx 400, loss 0.07106823474168777, Training set acc 0.9538094997406006, eval set accuracy 0.9373999834060669\n",
      "Epoch 3 - batch_idx 0, loss 0.6315268278121948, Training set acc 0.9543269872665405, eval set accuracy 0.9366999864578247\n",
      "Epoch 3 - batch_idx 100, loss 0.4103229343891144, Training set acc 0.9546441435813904, eval set accuracy 0.9378999471664429\n",
      "Epoch 3 - batch_idx 200, loss 0.1852245181798935, Training set acc 0.9559295177459717, eval set accuracy 0.9364999532699585\n",
      "Epoch 3 - batch_idx 300, loss 0.11765400320291519, Training set acc 0.9569478034973145, eval set accuracy 0.9373999834060669\n",
      "Epoch 3 - batch_idx 400, loss 0.09493402391672134, Training set acc 0.9579160213470459, eval set accuracy 0.9375\n",
      "Epoch 4 - batch_idx 0, loss 0.09584720432758331, Training set acc 0.9581998586654663, eval set accuracy 0.9375\n",
      "Epoch 4 - batch_idx 100, loss 0.8808577656745911, Training set acc 0.9587173461914062, eval set accuracy 0.9373999834060669\n",
      "Epoch 4 - batch_idx 200, loss 0.2277650535106659, Training set acc 0.9598190784454346, eval set accuracy 0.9386999607086182\n",
      "Epoch 4 - batch_idx 300, loss 0.22444172203540802, Training set acc 0.9608039855957031, eval set accuracy 0.9372999668121338\n",
      "Epoch 4 - batch_idx 400, loss 0.3821595013141632, Training set acc 0.9618557095527649, eval set accuracy 0.9386999607086182\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for idx, (x, y) in enumerate(train_loader):\n",
    "        state, loss = jit_train_step(state, x, y)\n",
    "        if idx % 100 == 0:  # evaluation\n",
    "            train_acc = eval_model(state, train_loader)\n",
    "            eval_acc = eval_model(state, eval_loader)\n",
    "            print(\"Epoch {} - batch_idx {}, loss {}, Training set acc {}, eval set accuracy {}\".format(\n",
    "              epoch, idx, loss, train_acc, eval_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993476cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
